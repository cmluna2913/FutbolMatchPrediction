{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../src/config.ini']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import requests\n",
    "import openpyxl\n",
    "\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "# some stuff I set up in a config file so I don't have to keep updating certain\n",
    "# variables in every script\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../src/config.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../src/config.ini']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('../src/config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output path is specified in the config.ini file\n",
    "output = Path(config['paths']['output'])\n",
    "# I want data for the 2022 through 2024 season\n",
    "yearly_directories = [Path(output/f\"mls_{year}\") for year in range(2022,2025)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output directory and sub-directories if doesnt exist\n",
    "for directory in yearly_directories+[output]:\n",
    "    try:\n",
    "        assert directory.exists()\n",
    "    except:\n",
    "        os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will be web-scraping alot, so I made this function as a result\n",
    "\n",
    "def get_html_data(url, parser='html.parser') -> bs4.BeautifulSoup:\n",
    "    '''\n",
    "    Extract html data from specified url and return a bs4 object.\n",
    "    Parser can be specified if needed. Default is html.parser.\n",
    "    '''\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    soup = BeautifulSoup(html_content, parser)\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I ended up using this a lot in the end\n",
    "def get_table_data_from_html(soup) -> list:\n",
    "    '''\n",
    "    Extract tables from bs4 object and return a list of dataframes.\n",
    "    '''\n",
    "    \n",
    "    # get all tables in the html\n",
    "    tables = soup.findAll('table')\n",
    "\n",
    "    # create dfs for each table and append each one to a list\n",
    "    dfs_from_tables = []\n",
    "    for table in tables:\n",
    "        dfs_from_tables.append(pd.read_html(StringIO(str(table)))[0])\n",
    "    \n",
    "    return dfs_from_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_player_match_data(excel_file_path) -> tuple[pd.DataFrame, list]:\n",
    "    '''\n",
    "    Input an excel file of the copied tables from the websites and return a tuple\n",
    "    where the first element is a dataframe of all player data and the second element\n",
    "    is a list of URLs where extraction failed. This attempts to extract data from\n",
    "    the failed URLs once more before completing.\n",
    "    '''\n",
    "    # get year from directory the excel file is in\n",
    "    year = os.path.dirname(excel_file_path).split('\\\\')[-1].replace('mls_', '')\n",
    "    # extract all links from the excel file\n",
    "    # returned as a df in case of future use with an idea I had\n",
    "    player_links = get_all_player_match_data_links(excel_file_path, year)\n",
    "    # initialize empty df for all player data\n",
    "    player_data_df = pd.DataFrame()\n",
    "    \n",
    "    # generate the initial player data df and failed links list\n",
    "    player_data_df, failed_links = generate_player_df(list(player_links['stat_link']), player_data_df)\n",
    "    \n",
    "    # if first round has failed links, retry them\n",
    "    # sometimes links failed for some reason but rerunning them seemed to fix it\n",
    "    if len(failed_links)>0:\n",
    "        player_data_df, failed_links = generate_player_df(failed_links, player_data_df)\n",
    "\n",
    "    return player_data_df, failed_links \n",
    "\n",
    "def generate_player_df(player_links, df=pd.DataFrame()) -> tuple[pd.DataFrame, list]:\n",
    "    '''\n",
    "    Given an iterable of individual player links (and a df to modify), return a tuple (DataFrame, list)\n",
    "    where the dataframe contains all player data and the list contains URLs with failed extractions.\n",
    "    '''\n",
    "    # get total number of players for alive_bar\n",
    "    total_players = len(player_links)\n",
    "    # to append failed links to a list\n",
    "    failed_links = []\n",
    "    \n",
    "    # progress bar, force_tty=True might be needed depending if animations don't show for you\n",
    "    # I used jupyter notebooks, so I needed this\n",
    "    with alive_bar(total_players, force_tty=True) as bar:\n",
    "        for player_url in player_links:\n",
    "            # limited to 10 requests a minute per website rules :(\n",
    "            # I set this as 7 seconds just to be safe but you can adjust as needed\n",
    "            # in the config file\n",
    "            time.sleep(int(config['other']['request_time_limit']))\n",
    "            \n",
    "            # if you get a player df, append it to df, otherwise add to failed links\n",
    "            temp_df = attempt_data_extraction(player_url)\n",
    "            if type(temp_df)==type(None):\n",
    "                failed_links.append(player_url)\n",
    "            else:\n",
    "                df = pd.concat([df, temp_df], ignore_index=True)\n",
    "            bar()\n",
    "    # return df and failed links\n",
    "    return df, failed_links\n",
    "\n",
    "def attempt_data_extraction(url) -> pd.DataFrame | None:\n",
    "    '''\n",
    "    Attempt to get a player data dataframe. Return the df if successful,\n",
    "    otherwise return None.\n",
    "    '''\n",
    "    # try to get a player_df and return it\n",
    "    try:\n",
    "        player_df = get_player_data_df(url)\n",
    "        return player_df\n",
    "    # otherwise, let the user know and return nothing\n",
    "    except:\n",
    "        print(f\"Could not get player data for {url}\")\n",
    "        return None\n",
    "\n",
    "def get_all_player_match_data_links(excel_file_path, year) -> pd.DataFrame:\n",
    "    '''\n",
    "    Extract all links from the excel that is a copy of the website data. Return\n",
    "    a dataframe of the player names and URLs.\n",
    "    '''\n",
    "    # read excel file\n",
    "    all_players = pd.read_excel(excel_file_path)\n",
    "    wb = openpyxl.load_workbook(excel_file_path)\n",
    "    sheets = wb.sheetnames\n",
    "    ws = wb[sheets[0]]\n",
    "    # get hyper links from file\n",
    "    # in this situation, links didn't start until +2 and are found in column 37\n",
    "    # this may need to be adjusted depending on how the data comes out\n",
    "    all_players['stat_link'] = [ws.cell(row=i+2, column=37).hyperlink.target for i in range(all_players.shape[0])]\n",
    "    # save all links out\n",
    "    all_players[['Player', 'stat_link']].to_csv(output/f'mls_{year}' / 'player_links.csv', index=False)\n",
    "    \n",
    "    # return df of just the player name and stat link\n",
    "    return all_players[['Player', 'stat_link']]\n",
    "    \n",
    "    \n",
    "def get_player_data_df(url) -> pd.DataFrame:\n",
    "    '''\n",
    "    Using the URL for the individual player, get the first table on the website\n",
    "    and return it as a dataframe.\n",
    "    '''\n",
    "    # get html data from url\n",
    "    player_data_html = get_html_data(url)\n",
    "    # generate tables list\n",
    "    player_df = get_table_data_from_html(player_data_html)\n",
    "    # add player url to data in first df in list\n",
    "    # the lists always had 1 df but would break unless I left it as-is\n",
    "    player_df[0]['player'] = player_url\n",
    "    \n",
    "    return player_df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Season - 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this url gives me a list of all players in the current league\n",
    "base_url = 'https://fbref.com/en/comps/22/Major-League-Soccer-Stats'\n",
    "\n",
    "# this page gives me a bunch of tables for team stats in the current moment\n",
    "# not sure how much of the data here will be useful, but I'll grab it just-in-case\n",
    "html = get_html_data(base_url + 'players/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_stat_dfs = get_table_data_from_html(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table names in the html\n",
    "tables = ['eastern_conference',\n",
    "          'western_conference',\n",
    "          'squad_standard_stats',\n",
    "          'squad_goalkeeping',\n",
    "          'squad_advanced_goalkeeping',\n",
    "          'squad_shooting',\n",
    "          'squad_passing',\n",
    "          'squad_pass_types',\n",
    "          'squad_goal_and_shot_creation',\n",
    "          'squad_defensive_actions',\n",
    "          'squad_possession',\n",
    "          'squad_playing_time',\n",
    "          'squad_miscellaneous_stats']\n",
    "\n",
    "# table pairings based on index in the list\n",
    "pairs = list(zip([i for i in range(0,26,2)], [i for i in range(1,27,2)]))\n",
    "\n",
    "# create csv files for each table\n",
    "for index, pair in enumerate(pairs):\n",
    "    pair_df = pd.concat([team_stat_dfs[pair[0]], team_stat_dfs[pair[1]]],axis=1)\n",
    "    pair_df.to_csv(team_stats/f'{tables[index]}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Player Data\n",
    "I had to copy and paste the \"Player Standard Stats\" table from this url (https://fbref.com/en/comps/22/stats/Major-League-Soccer-Stats) since I failed to do so with bs4. I saved it\n",
    "out as an excel file. I just want to extract the urls that go directly to the players stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_data_2022_df, failed_links_2024 = get_all_player_match_data(config['paths']['all_players_2023'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_data_2024_df.to_csv(output/\"mls_2024\"/\"all_player_data_2024.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRIOR YEARS\n",
    "I recommend running each year/season individually. At the time of creating this script,\n",
    "the website has a limit of 10 requests/minute. As a result, I had to stall with a timer\n",
    "of 7 seconds before running each request. This causes each year to take ~2 hours just to\n",
    "get the individual player performance data for each match. It would be best to finish\n",
    "tackling a year before moving onto the next. Depending on your PC, you might also need\n",
    "to take resources into account as well and save each year out before moving onto the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023 Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_data_2022_df, failed_links_2023 = get_all_player_match_data(config['paths']['all_players_2023'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_data_2023_df.to_csv(output/\"mls_2023\"/\"all_player_data_2023.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022 Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on 129: Could not get player data for https://fbref.com/en/players/1a160ea3/matchlogs/2022/summary/Geoff-Cameron-Match-Logs\n",
      "|████████████████████████████████████████| 784/784 [100%] in 1:43:27.9 (0.13/s) \n",
      "|████████████████████████████████████████| 1/1 [100%] in 7.8s (0.13/s)          \n"
     ]
    }
   ],
   "source": [
    "player_data_2022_df, failed_links_2022 = get_all_player_match_data(config['paths']['all_players_2022'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_data_2022_df.to_csv(output/\"mls_2022\"/\"all_player_data_2022.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting More\n",
    "Work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2024 = pd.read_csv(config['paths']['all_player_data'])\n",
    "# df_2023 = pd.read_csv(config['paths']['all_player_data_2023'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2024 = df_2024[df_2024['Unnamed: 2_level_0']==\"MLS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_dates = df_2024['Unnamed: 0_level_0'].value_counts().keys()\n",
    "for date in game_dates:\n",
    "    os.mkdir(output/'games'/f\"{date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_so_far = []\n",
    "\n",
    "for game_date in game_dates:\n",
    "    try:\n",
    "        game_data_filtered = df_2024[df_2024['Unnamed: 0_level_0']==game_date].copy()\n",
    "        game_data_filtered['game_key'] = game_data_filtered.apply(lambda x: f\"{x['Unnamed: 0_level_0']} {' vs '.join(sorted([x['Unnamed: 6_level_0'], x['Unnamed: 7_level_0']]))}\",\n",
    "                                                                    axis=1)\n",
    "        games_during_date = game_data_filtered['game_key'].value_counts().keys()\n",
    "        for game in games_during_date:\n",
    "            individual_game = game_data_filtered[game_data_filtered['game_key']==game]\n",
    "            individual_game.to_csv(output/'games'/f\"{game_date}\"/f\"{game}.csv\", index=False)\n",
    "            games_so_far.append(game)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_schedule_df = pd.DataFrame(games_so_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_schedule_df['date'] = game_schedule_df[0].apply(lambda x: x[:10])\n",
    "game_schedule_df['team1'] = game_schedule_df[0].apply(lambda x: x[10:].split(' vs ')[0])\n",
    "game_schedule_df['team2'] = game_schedule_df[0].apply(lambda x: x[10:].split(' vs ')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_schedule_df.drop(columns=0,inplace=True)\n",
    "game_schedule_df.to_csv(output/'game_schedule_2024.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FUTBOL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
